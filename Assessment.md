# Iris-dataset-technical-test
Technical test Data Science Internship

Objective: The goal of this exercise is to evaluate your proficiency in Python programming,
usage of GitHub, and SQL skills using a real-world dataset (Iris dataset). You will be working
with a dataset to perform data manipulation, analysis, and visualization.

Instructions:
1. GitHub Repository Setup:
o Create a github repository and add all the work.

2. Python Programming:
o Download the Iris dataset (iris.csv) and place it in the repository.
o Create a script to perform the following tasks:
• Load the Iris dataset into a Pandas DataFrame.
• Conduct exploratory data analysis (EDA) to understand the data.
• Implement a function calculate_statistics that calculates basic
statistics (mean, median, standard deviation) for a specified column in
the dataset.
• Create a plot (using Matplotlib or Seaborn) to visualize a relevant
aspect of the data.

3. SQL Skills:
o Write SQL queries to answer the following questions (apply these questions
to the Iris dataset):
• Retrieve the top 5 rows from the dataset.
• Calculate the total number of unique values in the 'species' column.
• Find the average value of the 'sepal_length' column for each species.

4. Documentation:
o Provide a brief explanation of your approach to solving the Python and SQL
tasks.
o Document any assumptions or considerations you made during the analysis.
o Include the plot generated in the Python script and describe its insights.

Submission:
• Commit your changes to your local repository.
• Push the changes to your forked GitHub repository.
• Share the link to your forked repository and any additional instructions for reviewing
your solution.
Evaluation Criteria:
• Correctness of Python code and SQL queries.
• Clarity and organization of code.
• Quality and relevance of data analysis.
• Effective use of version control (GitHub).
• Completeness and clarity of documentation.

# Solution

In the scripts folder, there is the file main.ipynb where we will see the code that solves the test proposed in section 2, Python Programming, as well as the documentation and explanation of the results obtained.

Also included are the SQL queries applied to the dataset as well as the result obtained by applying these queries in MySQL Workbench to the iris table (where the entire dataset is located).

In the plots folder, all the graphs generated in the main.ipynb file within scripts are collected.

In the sql folder, we find the file sql_queries.sql which we can open with MySQL Workbench and execute the queries directly to see the results in an SQL environment.

In requirements, we have a list that enumerates all the resources used for the resolution of this technical test.
